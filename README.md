<div align="center">
    <h1>SCIENTIFIC PAPER TRANSLATIONESE ANALYSER</h1>
</div>

 <div align="center">
    <h2>Программа для анализа феномена Translationese в переводах научных статей с английского языка на русский</h4>
</div>

Считается, что переведенные тексты обладают рядом уникальных особенностей, отличающих их от оригинальных текстов на целевом языке. Эти особенности, называемые общим терменом **Translationese**, проявляются в переводе в виде специфических индикаторов. Для выявления этих индикаторов принято проводить сравнительный анализ на материале корпусов переведенных и оригинальных текстов одного жанра.

Данная программа предназначена для подсчета и анализа этих индикаторов **в переводах научных статей с английского языка на русский.** 

* **
## Оглавление

1. [Установка и запуск программы](#установка-и-запуск-программы)
   - [Инструкция для пользователей, у которых есть опыт работы с GitHub](#инструкция-для-пользователей-у-которых-есть-опыт-работы-с-github)  
   - [Инструкция для пользователей без опыта работы с GitHub](#инструкция-для-пользователей-без-опыта-работы-с-github)

2. [Краткое описание возможностей программы](#описание-возможностей-программы)
3. [Описание анализируемых индикаторов](#программа-предоставляет-возможности-анализа-следующих-индикаторов) 
   - [Simplification](#simplification)  
   - [Normalization](#normalization)  
   - [Explicitation](#explicitation)  
   - [Interference](#interference)  
   - [Другие индикаторы](#другие-индикаторы)  

* **

## Установка и запуск программы

Для того чтобы воспользоваться программой, на Вашем компьютере должен быть установлен интерпретатор **Python** и **среда разработки (IDE)**. Чтобы установить их, пожалуйста, внимательно прочитайте инструкцию о том, как это сделать, [здесь](https://github.com/serovaolesya/sci_papers_translationese/blob/main/README_HOW_TO_INSTALL_PYHTHON.md) и выполните все описанные в ней шаги.

* **

### Инструкция для пользователей, у которых есть опыт работы с GitHub

**1.** **Клонируйте репозиторий на свой компьютер:**
    
    git clone git@github.com:serovaolesya/sci_papers_translationese.git
**2.** **Откройте склонированный проект в любом IDE и обязательно установите виртуальное окружение:**

Команда для Windows:

    python -m venv venv
    
Команда для Linux и macOS:

    python3 -m venv venv

После выполнения этой команды в директории проекта появится папка venv (от virtual environment, «виртуальное окружение»), в ней хранятся служебные файлы. В этой же директории будут храниться все библиотеки и модули, используемые в приложении.

**3.** **Активируйте виртуальное окружение. Для этого, находясь в корневой директории проекта, введите в терминал команду:**

Команда для Windows:

    source venv/Scripts/activate
или

    venv/Scripts/activate


Для Linux и macOS:

     source venv/bin/activate

**4.** **Обязательно установите зависимости (библиотеки и модули) приложения, без них оно работать не будет:**

Из корневой директории проекта введите следующую команду:

    pip install -r requirements.txt

**5.** **Запустите программу:**

**В корневой директории находится файл `start_analysis.py`, ОТКРОЙТЕ его двойным кликом и запустите (треугольная кнопка ▶️ RUN).**

**ЛИБО** через терминал **введите команду:**

    python start_analysis.py

**5.** **Далее следуйте инструкциям на экране, чтобы воспользоваться приложением.**

* **
### Инструкция для пользователей без опыта работы с GitHub


**1.** **Скачайте ZIP-архив с программой [по этой ссылке](https://github.com/serovaolesya/sci_papers_translationese/archive/refs/heads/main.zip).**

**2.** **Распакуйте ZIP-архив с программой.** Возможно, при распаковке программа предложит Вам распаковать содержимое архива в папку с одноименным именем, удалите название архива и распакуйте содержимое в папку на уровень выше:
<div align="center">
  <img src="https://github.com/user-attachments/assets/c17ad8d4-80b4-4712-815b-a84f53d02653" alt="Project structure">
</div>

**3.** **Откройте установленную ранее среду разработки (IDE) (об установке Python и IDE можно прочитать [здесь](https://github.com/serovaolesya/sci_papers_translationese/blob/main/README_HOW_TO_INSTALL_PYHTHON.md)).** 

Мы рекомендуем воспользоваться бесплатной версией IDE - **PyCharm Community Edition**.

Скорее всего, IDE сам предложит вам открыть интересующий Вас проект. Либо в приложении **PyCharm Community Edition** нужно будет открыть раздел **"File"** > **"Open"** и в открывшемся окне выбрать папку с распакованным приложением. Когда Вы откроете папку с приложением, Вы должны увидеть такую структуру файлов:

<div align="center">
  <img src="https://github.com/user-attachments/assets/5f45c98e-c1b6-4bec-9260-22bc5a740eff" alt="Project structure">
</div>

**4.** **Откройте терминал IDE и введите следующую команду:**

**Команда для Windows:**

    python -m venv venv
    
**Команда для Linux и macOS:**

    python -m venv venv
    
<div align="center">
  <img src="https://github.com/user-attachments/assets/0641ce90-f59f-4b55-abaf-4a864e0f3833" alt="Project structure">
</div>

После выполнения этой команды в директории проекта появится папка venv (от virtual environment, **«виртуальное окружение»**), в ней хранятся служебные файлы. В этой же директории будут храниться все библиотеки и модули, используемые в приложении.


**5.** **Активируйте созданное виртуальное окружение venv. Для этого введите в терминал команду и нажмите `Enter`:**

**Команда для Windows:**
    source venv/Scripts/activate
    
**или**

    venv/Scripts/activate


**Для Linux и macOS:**

     source venv/bin/activate

**6.** **Далее введите следующую команду для установки библиотек и модулей приложения, без них оно работать не будет:**

Из корневой директории проекта введите следующую команду и подождите, пока все библиотеки скачаются и установятся:

    pip install -r requirements.txt


**7.** **Запустите программу:**

**В корневой директории находится файл `start_analysis.py`, ОТКРОЙТЕ его двойным кликом и запустите (треугольная кнопка ▶️ RUN).**

**ЛИБО** через терминал **введите команду:**

    python start_analysis.py

**8.** **Далее следуйте инструкциям на экране, чтобы воспользоваться приложением.**

* **
## Описание возможностей программы

**Программа предоставляет следующие возможности:**   

* **Предобработка текстов перед последующим анализом,** включающая себя удаление ссылок на литературу, а также проверку правильности разбиения текста на предложения и дальнейший подгон текста по объему, выбранному пользователем.
   - Ввод текста для предобоаботки может производиться либо сочетанием клавиш **Ctrl+V** через консоль, либо пользователь должен заранее добавить файлы с текстами для предобработки в одну из следующих папок в формате **.txt**:
        - **auth_texts** - папка для аутентичных текстов,
        - **mt_texts** - папка для текстов МП,
        - **ht_texts** - папка для переводов, сделанных человеком.

* **Анализ текстов, вводимых пользователем по пяти группам индикаторов феномена Translationese.**
  -  Ввод текстов может производиться либо сочетанием клавиш **Ctrl+V** через консоль, либо пользователь должен заранее добавить подготовленные для анализа файлы в одну из следующих папок в формате **.txt**:
        -  **auth_ready** - папка для аутентичных текстов,
        -  **mt_ready** - папка для текстов МП,
        -  **ht_ready** - папка для переводов, сделанных человеком.
          
    Если пользователь воспользовался встроенной в приложение функцией по предобработке текста, то предобработанные и готовые для анализа тексты уже будут находится в одной из указанных папок.

* **Просмотр информации о выбранном тексте в корпусе.**

* **Просмотр информации обо всем корпусе текстов.**
  
* **Также в приложение встроены функции создания паспорта текста, морфологической и синтаксической разметки текста.**


* **
## Программа предоставляет возможности анализа следующих индикаторов:

###  **Simplification** 
Универсалия основывается на утверждении, что тексты перевода структурно и лексически проще аутентичных текстов, представлена следующими индикаторами:


1.  **Лексическая плотность** (lexical density)  — отношение знаменательных слов (а именно существительных, прилагательных, глаголов и наречий) к общему количеству слов.
2.  **Лексическое разнообразие** (Lexical Variety) - отношение типов (уникальных слов) ко всем словарным токенам текста.
    
    *   **Рассчитываются следующие версии данного индикатора:**
        1.  $TTR = \frac{V}{N}$, где V - количество уникальных типов, а N - количество словарных токенов.
           
        2.  $Log\text{-}TTR = \frac{\log(V)}{\log(N)}$, где  V - количество уникальных типов, а N - количество словарных токенов.

        3.  $Modified\text{-}TTR = \frac{100 \times \log(N)}{1 - \frac{V_1}{V}}$, где V - количество уникальных типов, а N - количество словарных токенов, V1 — количество уникальных типов, встречающихся только один раз.
           
3.  **Средняя длина слов в символах** (mean word length in characters).
    
4.  **Средняя длина слов в слогах** (mean word length in syllables).
    
5. **Средняя длина предложений в символах** (mean sentence length in characters).

6. **Средняя длина предложений в токенах** (mean sentence length in tokens).
    
7. **Средний ранг слов в тексте** (mean word rank) — используется список из 6000 наиболее частотных слов, взятый из [Нового частотного словаря русской лексики](http://dict.ruslang.ru/freq.php). Ранг слова равен его порядковому номеру. Максимальный ранг равен 5000 (поскольку некоторые слова в списке имеют одинаковый ранг).
    *   **Рассчитываются следующие версии данного индикатора:**
      1. Словам, отсутствующим в списке, присваивается уникальный наивысший ранг 6000.
      2. Слова, отсутствующие в списке, полностью игнорируются.
    
8.  **Частотность 50-ти наиболее употребимых слов** (50 most frequent words).

* **

### **Normalization**
Универсалия основывается на утверждении, что в переводах используются более нормализованные грамматические структуры, большее количество устойчивых выражений, а также имеется тенденция к избеганию лексических повторов знаменательных частей речи. Представлена следующими индикаторами:

1.  **Повторяемость** (repetition) - соотношение знаменательных слов (а именно глаголов, существительных, прилагательных и наречий), встречающихся в тексте более одного раза, к общему количеству токенов в тексте. Глагол-связка «быть/являться» исключена из подсчетов.

2.  **PMI (pointwise mutual information)** для всех биграммов в тексте. PMI биграммов $w_1w_2$ ее PMI вычисляется как:
    
    $I(w_1, w_2) = \log_2\left(\frac{P(w_1, w_2)}{P(w_1) \cdot P(w_2)}\right)$,

      где $P(w_1, w_2)$ - вероятность совместного появления пары слов вместе, $P(w_1)$ и $P(w_2)$ - вероятности появлений отдельных слов.
3. **Биграммы с PMI > 0** (Threshold PMI=0) - нормализованное количество биграмм с PMI > 0.

* **

### **Explicitation**
Универсалия подразумевает, что в текстах перевода есть тенденция к эксплицитному выражению элементов, имплицитно подразумевающихся в тексте оригинала, что приводит к повышенной логической связности текста. Представлена следующими индикаторами:

1. **Эксплицитное называние** (explicit naming) - отношение личных местоимений к именованным сущностям.

2. **Частота именованных сущностей из одного токена** (single naming) относительно всех именованных сущностей в тексте.

3. **Средняя длина именованных сущностей в токенах** (mean multiple naming). 

4. **Дискурсивные маркеры научного текста** (discourse markers). Анализ включает в себя подсчет нормализованных частот следующих типов ДМ научного текста _(в скобках даны примеры)_:
    - **Введение в тему** _(речь пойдет о, будет рассмотрено, мы рассмотрим)_,
    - **Порядок следования информации** _(во-первых, прежде всего, наконец)_,
    - **Иллюстративный материал** _(из таблицы видно, на рисунке, данные представлены)_,
    - **Порядок расположения материала** _(как говорилось выше, об этом речь пойдет, как было показано ранее)_,
    - **Вывод/заключение** _(поэтому, следовательно, следует вывод)_,
    - **Введение новой/дополнительной информации** _(кстати говоря, причем, кроме того)_,
    - **Повтор/конкретизация информации** _(то есть, другими словами, потому что)_,
    - **Противопоставление** _(однако, тем не менее, между тем)_,
    - **Введение примеров** _(например, такие как, к примеру)_,
    - **Мнение автора** _(на наш взгляд, по нашему мнению, как нам кажется)_,
    - **Категоричная оценка** _(конечно, разумеется, несомненно)_,
    - **Менее категоричная оценка** _(возможно, вероятно, наверно)_,
    - **Призыв читателя к действию** _(см., ср.)_,
    - **Совместное действие** _(рассмотрим, обозначим, перейдем к)_,
    - **Акцентирование внимания** _(необходимым представляется отметить, следует подчеркнуть, отметим, что)_,
    - **Отсылка к фоновым знаниям** _(не секрет, что, общеизвестно, как известно)_.

* **

### **Interference**
Универсалия подразумевает тенденцию к переносу структурных характеристик исходного языка в текст перевода, педставлена следующими индикаторами:

1. **POS n-граммы** (POS n-grams), а именно нормализованные частоты частеречных **униграммов**, **биграммов** и **триграммов**. Специальные обозначения **S_START** и **S_END** добавлены для обозначения начала и конца предложений, что позволяет захватывать биграммы и триграммы, находящиеся на границах предложений.

     **Таблица используемых в анализе POS n-граммов:**

<div align="center">
  <img width="680" alt="image" src="https://github.com/user-attachments/assets/72de67d8-6ca6-4365-8e95-117be5fe33f9" alt="Project structure">
</div>

2. **Буквенные n-граммы** (character n-grams), а именно нормализованные частоты буквенных **униграммов**, **биграммов** и **триграммов**. Специальные символы **<** и **>** добавлены для обозначения начала и конца слов, что позволяет захватывать биграммы и триграммы, находящиеся на границах слов, и корректно обрабатывать префиксы и суффиксы.

3. **Триграммы слов с функциональными (служебными) словами** (contextual function words in trigrams).

     **Учитываются следующие триграммы**:
      - Триграммы состоящие из одного служебного слова и двух других слов (заменённых на их POS-теги),
      - Триграммы состоящие из двух служебных слов и одного другого слова (заменённого на его POS-тег),
      - Триграммы состоящие из трех служебных слов.


4. **Позиционная частота токенов на разных позициях в предложении** (positional token frequency), а именно нормализованная частота токенов, встречающихся на **первой**, **второй**, **третьей с конца**, **предпоследней** и **последней** позициях в предложении. Предложения короче пяти токенов из анализа исключаются, знаки препинания считаются полноценными токенами.

* **

### **Другие индикаторы**

1. **Нормализованная частота функциональных (служебных) слов** (function words).
   
3. **Нормализованная частота личных и притяжательных местоимений** (pronouns).

3. **Нормализованная частота пунктуационных знаков** (?!:;–()[]«»‘’“”/,.) (punctuation)
           **1)** относительно всех знаков и
           **2)** относительно всех токенов в тексте.

5. **Соотношение глаголов в форме страдательного залога к общему количеству глаголов** (ratio of passive forms to all verbs).
   
* **
### При написании программы были использованы следующие инструменты для работы с естественным языком: 

- [Pymorphy](https://github.com/pymorphy2/pymorphy2) — морфологический анализатор для русского языка.
- [Natasha](https://github.com/natasha/natasha) — набор инструментов для обработки текстов на русском языке.
- [NLTK (Natural Language Toolkit)](https://www.nltk.org/) — библиотека для работы с естественными языками на Python.



# Приятного пользования!

   
